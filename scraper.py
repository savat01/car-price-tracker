from playwright.sync_api import sync_playwright
import json
import os
import re


def extract_price_only_before_million(text):
    """ÙŠØ³ØªØ®Ø±Ø¬ Ø§Ù„Ø±Ù‚Ù… ÙÙ‚Ø· Ø§Ù„Ø°ÙŠ ÙŠØ£ØªÙŠ Ù‚Ø¨Ù„ ÙƒÙ„Ù…Ø© Ù…Ù„ÙŠÙˆÙ†"""
    if not text:
        return None

    match = re.search(r'(\d+(\.\d+)?)\s*Ù…Ù„ÙŠÙˆÙ†', text)
    if match:
        return match.group(1)  # Ø§Ù„Ø±Ù‚Ù… ÙÙ‚Ø· ÙƒÙ€ string
    return None


def scroll_and_wait(page, scrolls=3):
    """Ø§Ù„ØªÙ…Ø±ÙŠØ± ÙˆØ§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ±"""
    for i in range(scrolls):
        page.evaluate(f"window.scrollTo(0, {i * 500});")
        page.wait_for_timeout(1500)
    page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
    page.wait_for_timeout(2000)
    page.evaluate("window.scrollTo(0, 0);")
    page.wait_for_timeout(1000)


def scrape_dzairauto(page):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† DzairAuto"""
    cars = []
    try:
        url = "https://dzairauto.net/Voitures-occasion-avendre"
        print(f"\nðŸ“ DzairAuto: {url}")
        page.goto(url, wait_until="domcontentloaded", timeout=30000)
        page.wait_for_timeout(4000)
        scroll_and_wait(page)
        
        selectors = [
            "div.d-flex.justify-content-end.text-right",
            "div[class*='d-flex'][class*='justify-content-end'][class*='text-right']",
            ".d-flex.justify-content-end.text-right"
        ]
        
        listings = []
        for selector in selectors:
            try:
                elements = page.query_selector_all(selector)
                if elements and len(elements) > 0:
                    listings = elements
                    print(f"  ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(elements)} Ø¹Ù†ØµØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {selector}")
                    break
            except Exception:
                continue
        
        if not listings:
            print("  âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø¹Ù†Ø§ØµØ± ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ class='d-flex justify-content-end text-right'")
            return []
        
        for idx, listing in enumerate(listings[:50], 1):
            try:
                content = listing.inner_text().strip()
                html_content = listing.inner_html()
                price = extract_price_only_before_million(content)
                if not price:
                    price = "ØºÙŠØ± Ù…ØªÙˆÙØ±"
                
                name = None
                
                try:
                    title_elem = listing.evaluate("""
                        el => {
                            const parent = el.closest('div[class*="card"], article, div[class*="item"]');
                            if (parent) {
                                const title = parent.querySelector('h5, h4, h3, h2, a, .title, [class*="title"]');
                                return title ? title.innerText.trim() : null;
                            }
                            return null;
                        }
                    """)
                    if title_elem:
                        name = title_elem
                except:
                    pass
                
                if not name:
                    # Ø§Ø³ØªØ®Ø¯Ù… Ø£ÙˆÙ„ Ø³Ø·Ø± Ù…Ù† Ø§Ù„Ù†Øµ ÙƒØ§Ø³Ù… ØªÙ‚Ø±ÙŠØ¨ÙŠ
                    lines = content.split('\n')
                    name = lines[0].strip()[:100] if lines else content[:100]
                
                car_data = {
                    "content": content,
                    "price": price,
                    "source": "DzairAuto"
                }
                
                if name:
                    car_data["name"] = name
                
                if html_content:
                    car_data["html"] = html_content[:500]
                
                cars.append(car_data)
                
                if idx <= 3:
                    print(f"  [{idx}] Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {content[:80]}...")
                    print(f"      Ø§Ù„Ø³Ø¹Ø±: {price}")
                
            except Exception as e:
                print(f"  Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†ØµØ± {idx}: {e}")
                continue
        
        print(f"  âœ… Ø¬Ù…Ø¹Øª {len(cars)} Ø¹Ù†ØµØ± Ù…Ù† DzairAuto")
        return cars
    except Exception as e:
        print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ DzairAuto: {e}")
        import traceback
        traceback.print_exc()
        return []


def gather_prices_from_dzairauto(headless=True):
    """Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª Ù…Ù† DzairAuto ÙÙ‚Ø·"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        context = browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        cars = scrape_dzairauto(page)
        browser.close()
        return cars


def save_to_json(data, filename="latest_car_prices.json"):
    os.makedirs("car-price-tracker/data", exist_ok=True)
    filepath = os.path.join("car-price-tracker", "data", filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return filepath


if __name__ == "__main__":
    print("=" * 70)
    print("ðŸš— Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ DzairAuto")
    print("=" * 70)

    all_cars = gather_prices_from_dzairauto(headless=True)  # ÙÙŠ GitHub ActionsØŒ Ù†Ø³ØªØ®Ø¯Ù… headless=True

    print("\n" + "=" * 70)
    if all_cars:
        print(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(all_cars)} Ø³ÙŠØ§Ø±Ø© Ù…Ù† DzairAuto!")
        for i, car in enumerate(all_cars[:10], 1):
            print(f"  {i}. {car.get('name', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
            print(f"     Ø§Ù„Ø³Ø¹Ø±: {car.get('price', 'ØºÙŠØ± Ù…ØªÙˆÙØ±')}")
    else:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø¬Ù…Ø¹ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª.")

    filepath = save_to_json(all_cars)
    print(f"\nðŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ: {filepath}")
